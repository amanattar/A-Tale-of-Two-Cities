{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> A Tale of Two cities</h1>\n",
    "<h2 align=\"center\">Clustering the Neighborhoods of Mumbai andÂ London</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">Amanul Rahiman Shamshuddin Attar\n",
    "<br>\n",
    "<br>\n",    
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Mumbai and London are the most popular cities in the world. These two cities have major history in past. It has changed over the years and we now take a look at how cities have grown.\n",
    "Mumbai and London are quite popular tourist and vacation destination for people around the world. They are diverse and multicultural and offer a wide variety of experiences that is widely sought after. We try to group the neighborhoods of Mumbai and London respectively and draw insights to what they look like now.\n",
    "\n",
    "# 2. Business Problem\n",
    "\n",
    "The aim is to help tourists choose their destinations depending on the experiences that the neighborhoods have to offer and what they would want to have. This also helps people make decisions if they are thinking about migrating to Mumbai or London or even if they want to relocate neighborhoods within the city. Our findings will help stakeholders make informed decisions and address any concerns they have including the different kinds of cuisines, provision stores and what the city has to offer.\n",
    "\n",
    "\n",
    "# 3. Data Description\n",
    "\n",
    "We require geolocation data for both Mumbai and London. Postal codes in each city serve as a starting point. Using Postal codes, we use can find out the neighborhoods, boroughs, venues and their most popular venue categories.\n",
    "\n",
    "\n",
    "## 3.1 Mumbai\n",
    "\n",
    "To derive our solution, We scrape our data from \n",
    "https://en.wikipedia.org/wiki/List_of_neighbourhoods_in_Mumbai\n",
    "\n",
    "This wikipedia page has information about all the neighbourhoods, we limit it South Mumbai.\n",
    "\n",
    "1. *borough* : Name of Neighbourhood\n",
    "2. *town* : Name of borough\n",
    "3. *latitude* : Latitude for Neighbourhood\n",
    "4. *longitude* : Longitude for Neighbourhood\n",
    "\n",
    "## 3.2 London\n",
    "\n",
    "To derive our solution, We scrape our data from https://en.wikipedia.org/wiki/List_of_areas_of_London\n",
    "\n",
    "This wikipedia page has information about all the neighbourhoods, we limit it London.\n",
    "\n",
    "1. *borough* : Name of Neighbourhood\n",
    "2. *town* : Name of borough\n",
    "3. *post_code* : Postal codes for London.\n",
    "\n",
    "This wikipedia page lacks information about the geographical locations. To solve this problem we use ArcGIS API\n",
    "\n",
    "### 3.3 ArcGIS API\n",
    "\n",
    "ArcGIS Online enables you to connect people, locations, and data using interactive maps. Work with smart, data-driven styles and intuitive analysis tools that deliver location intelligence. Share your insights with the world or specific groups. \n",
    "\n",
    "More specifically, we use ArcGIS to get the geo locations of the neighbourhoods of London. The following columns are added to our initial dataset which prepares our data. \n",
    "\n",
    "4. *latitude* : Latitude for Neighbourhood\n",
    "5. *longitude* : Longitude for Neighbourhood\n",
    "\n",
    "## 3.4 Foursquare API Data\n",
    "\n",
    "We will need data about different venues in different neighbourhoods of that specific borough. In order to gain that information we will use \"Foursquare\" locational information. Foursquare is a location data provider with information about all manner of venues and events within an area of interest. Such information includes venue names, locations, menus and even photos. As such, the foursquare location platform will be used as the sole data source since all the stated required information can be obtained through the API.\n",
    "\n",
    "After finding the list of neighbourhoods, we then connect to the Foursquare API to gather information about venues inside each and every neighbourhood. For each neighbourhood, we have chosen the radius to be 1000 meters.\n",
    "\n",
    "The data retrieved from Foursquare contained information of venues within a specified distance of the longitude and latitude of the postcodes. The information obtained per venue as follows:\n",
    "\n",
    "1. *Neighbourhood* : Name of the Neighbourhood\n",
    "2. *Neighbourhood Latitude* : Latitude of the Neighbourhood\n",
    "3. *Neighbourhood Longitude* : Longitude of the Neighbourhood\n",
    "4. *Venue* : Name of the Venue\n",
    "5. *Venue Latitude* : Latitude of Venue\n",
    "6. *Venue Longitude* : Longitude of Venue\n",
    "7. *Venue Category* : Category of Venue\n",
    "\n",
    "\n",
    "Based on all the information collected for both Mumbai and London, we have sufficient data to build our model. We cluster the neighbourhoods together based on similar venue categories. We then present our observations and findings. Using this data, our stakeholders can take the necessary decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating our model with the help of Python so we start off by importing all the required packages.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "```\n",
    "\n",
    "Package breakdown:\n",
    "\n",
    "\n",
    "*   *Pandas* : To collect and manipulate data in JSON and HTMl and then data analysis\n",
    "*   *requests* : Handle http requests\n",
    "*   *matplotlib*  : Detailing the generated maps\n",
    "*   *folium* : Generating maps of London and Paris\n",
    "* *sklearn* : To import Kmeans which is the machine learning model that we are using.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The approach taken here is to explore each of the cities individually, plot the map to show the neighbourhoods being considered and then build our model by clustering all of the similar neighbourhoods together and finally plot the new map with the clustered neighbourhoods. We draw insights and then compare and discuss our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data collection stage, we begin with collecting the required data for the cities of Mumbai and London. We need data that has the postal codes, neighbourhoods and boroughs specific to each of the cities.\n",
    "\n",
    "To collect data for Mumbai, we scrape the List of neighbourhoods in Mumbai wikipedia page to take the 1st table using the following code:\n",
    "\n",
    "```python\n",
    "url_mumbai = \"https://en.wikipedia.org/wiki/List_of_neighbourhoods_in_Mumbai\"\n",
    "wiki_mumbai_url = requests.get(url_mumbai)\n",
    "wiki_mumbai_data = pd.read_html(wiki_mumbai_url.text)\n",
    "wiki_mumbai_data = wiki_mumbai_data[0]\n",
    "wiki_mumbai_data\n",
    "```\n",
    "\n",
    "The data look like this :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Mumbai_Data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect data for London, we scrape the List of areas of London wikipedia page to take the 2nd table using the following code:\n",
    "\n",
    "```python\n",
    "url_london = \"https://en.wikipedia.org/wiki/List_of_areas_of_London\"\n",
    "wiki_london_url = requests.get(url_london)\n",
    "wiki_london_data = pd.read_html(wiki_london_url.text)\n",
    "wiki_london_data = wiki_london_data[1]\n",
    "wiki_london_data\n",
    "```\n",
    "\n",
    "The data looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/London_Data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Mumbai, We replace the spaces with underscores in the title.The *borough* column has numbers within square brackets that we remove using:\n",
    "```python\n",
    "wiki_mumbai_data.rename(columns=lambda x: x.strip().replace(\" \", \"_\"), inplace=True)\n",
    "```\n",
    "\n",
    "For London, We replace the spaces with underscores in the title.The *borough* column has numbers within square brackets that we remove using:\n",
    "\n",
    "```python\n",
    "wiki_london_data.rename(columns=lambda x: x.strip().replace(\" \", \"_\"), inplace=True)\n",
    "wiki_london_data['borough'] = wiki_london_data['borough'].map(lambda x: x.rstrip(']').rstrip('0123456789').rstrip('['))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both of our datasets, we need only the borough, neighbourhood, postal codes and geolocations (latitude and longitude). So we end up selecting the columns that we need by:\n",
    "\n",
    "```python\n",
    "df2 = wiki_london_data.drop( [ wiki_london_data.columns[0], wiki_london_data.columns[4], wiki_london_data.columns[5] ], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of our Datasets actually contain information related to all the cities in the country. We can narrow down and further process the data by selecting only the neighbourhoods pertaining to 'Mumbai' and 'London'\n",
    "\n",
    "```python\n",
    "df1.columns = [\"borough\", \"town\",\"latitude\",\"longitude\"]\n",
    "\n",
    "df2 = df2[df2['town'].str.contains('LONDON')]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Mumbai data set we get the latitude and longitude in the code so we dont have to work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Mumbai_Final_Data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking over our London dataset, we can see that we don't have the geolocation data. We need to extrapolate the missing data for our neighbourhoods. We perform this by leveraging the `ArcGIS API`. With the Help of `ArcGIS API` we can get the latitude and longitude of our London neighbourhood data. \n",
    "\n",
    "```python\n",
    "from arcgis.geocoding import geocode\n",
    "from arcgis.gis import GIS\n",
    "gis = GIS()\n",
    "```\n",
    "\n",
    "Defining London arcgis geocode function to return latitude and longitude\n",
    "\n",
    "```python\n",
    "def get_x_y_uk(address1):\n",
    "   lat_coords = 0\n",
    "   lng_coords = 0\n",
    "   g = geocode(address='{}, London, England, GBR'.format(address1))[0]\n",
    "   lng_coords = g['location']['x']\n",
    "   lat_coords = g['location']['y']\n",
    "   return str(lat_coords) +\",\"+ str(lng_coords)\n",
    "```\n",
    "\n",
    "Passing postal codes of london to get the geographical co-ordinates\n",
    "\n",
    "```python\n",
    "coordinates_latlng_uk = geo_coordinates_uk.apply(lambda x: get_x_y_uk(x))\n",
    "```\n",
    "\n",
    "We proceed with Merging our source data with the geographical co-ordinates to make our dataset ready for the next stage\n",
    "\n",
    "```python\n",
    "london_merged = pd.concat([df1,lat_uk.astype(float), lng_uk.astype(float)], axis=1)\n",
    "london_merged.columns= ['borough','town','post_code','latitude','longitude']\n",
    "london_merged\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"assets/London_Final_Data.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Both the datasets have been properly processed and formatted. Since the same steps are applied to both the datasets of Mumbai and London, we will be discussing the code for only the London dataset for simplicity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Visualizing the Neighbourhoods of Mumbai and London\n",
    "\n",
    "Now that our datasets are ready, using the `Folium` package, we can visualize the maps of Mumbai and London with the neighbourhoods that we collected.\n",
    "\n",
    "Neighbourhood map of Mumbai:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Neighbourhood_of_Mumbai.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbourhood map of London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Neighbourhood_of_London.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have visualized the neighbourhoods, we need to find out what each neighbourhood is like and what are the common venue and venue categories within a 100om radius. \n",
    "\n",
    "This is where `Foursquare` comes into play. With the help of `Foursquare` we define a function which collects information pertaining to each neighbourhood including that of the name of the neighbourhood, geo-coordinates, venue and venue categories.\n",
    "\n",
    "```python\n",
    "LIMIT=400\n",
    "\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=1000):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius,\n",
    "            LIMIT\n",
    "            )\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighbourhood', \n",
    "                  'Neighbourhood Latitude', \n",
    "                  'Neighbourhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)\n",
    "```\n",
    "\n",
    "Resulting data look like:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Mumbai_Neighbourhood_List.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are trying to find out what are the different kinds of venue categories present in each neighbourhood and then calculate the top 10 common venues to base our similarity on, we use the One Hot Encoding to work with our categorical datatype of the venue categories. This helps to convert the categorical data into numeric data.\n",
    "\n",
    "We won't be using label encoding in this situation since label encoding might cause our machine learning model to have a bias or a sort of ranking which we are trying to avoid by using One Hot Encoding.\n",
    "\n",
    "We perform one hot encoding and then calculate the mean of the grouped venue categories for each of the neighbourhoods.\n",
    "\n",
    "```python\n",
    "# One hot encoding\n",
    "Mumbai_venue_cat = pd.get_dummies(venues_in_Mumbai[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# Adding neighbourhood to the mix\n",
    "Mumbai_venue_cat['Neighbourhood'] = venues_in_Mumbai['Neighbourhood']\n",
    "\n",
    "# moving neghbourhood colun to the first column\n",
    "fixed_columns = [Mumbai_venue_cat.columns[-1]] + list(Mumbai_venue_cat.columns[:-1])\n",
    "\n",
    "# Grouping and calculating the mean\n",
    "Mumbai_grouped = Mumbai_venue_cat.groupby('Neighbourhood').mean().reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Mumbai_Grouped_Data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Top Venues in the Neighbourhoods\n",
    "\n",
    "In our next step, We need to rank and label the top venue categories in our neighborhood.\n",
    "\n",
    "Let's define a function to get the top venue categories in the neighbourhood\n",
    "\n",
    "```python\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]\n",
    "```\n",
    "\n",
    "There are many categories, we will consider top 10 categories to avoid data skew. \n",
    "\n",
    "Defining a function to label them accurately\n",
    "\n",
    "```python\n",
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighbourhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "```\n",
    "\n",
    "Getting the top venue categories in the neighbourhoods of Mumbai\n",
    "\n",
    "```python\n",
    "# create a new dataframe for Mumbai\n",
    "neighborhoods_venues_sorted_mumbai = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted_mumbai['Neighbourhood'] = Mumbai_grouped['Neighbourhood']\n",
    "\n",
    "for ind in np.arange(Mumbai_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted_mumbai.iloc[ind, 1:] = return_most_common_venues(Mumbai_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted_mumbai.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Mumbai_10_Common_Venue.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Model Building - KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the most exicitng part - **Model Building!** We will be using KMeans Clustering Machine learning algorithm to cluster similar neighbourhoods together. We will be going with the number of clusters as 5.\n",
    "\n",
    "```python\n",
    "# set number of clusters\n",
    "k_num_clusters= 5\n",
    "\n",
    "Mumbai_grouped_clustering = Mumbai_grouped.drop('Neighbourhood',1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans_mumbai = KMeans(n_clusters=k_num_cluster, random_state=0).fit(Mumbai_grouped_clustering)\n",
    "```\n",
    "\n",
    "Our model has labelled each of the neighbourhoods, we add the label into our dataset.\n",
    "\n",
    "```python\n",
    "neighborhoods_venues_sorted_mumbai.insert(0,'Cluster Labels',kmeans_mumbai.labels_ +1)\n",
    "```\n",
    "\n",
    "We then join Mumbai_merged with our neighbourhood venues sorted to add latitude & longitude for each of the neighborhood to prepare it for visualization.\n",
    "\n",
    "```python\n",
    "mumbai_data = df1\n",
    "\n",
    "mumbai_data = mumbai_data.join(neighborhoods_venues_sorted_mumbai.set_index('Neighbourhood'), on = 'borough')\n",
    "\n",
    "mumbai_data.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Mumbai_Merged_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Visualizing the clustered Neighbourhoods\n",
    "\n",
    "Our data is processed, missing data is collected and compiled. The Model is built. All that's remaining is to see the clustered neighbourhoods on the map. Again, we use `Folium` package to do so.\n",
    "\n",
    "We drop all the NaN values to prevent data skew\n",
    "\n",
    "```python\n",
    "mumbai_data_nonan = mumbai_data.dropna(subset=['Cluster Labels'])\n",
    "```\n",
    "\n",
    "Map of clustered neighbourhoods of Mumbai:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Mumbai_Cluster_Map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map of clustered neighbourhoods of London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/London_Cluster_Map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.1 Examining our Clusters\n",
    "\n",
    "We could examine our clusters by expanding on our code using the `Cluster Labels` column:\n",
    "\n",
    "Cluster 1\n",
    "\n",
    "```python\n",
    "mumbai_data_nonan.loc[mumbai_data_nonan['Cluster Labels'] == 1, mumbai_data_nonan.columns[[1] + list(range(5, mumbai_data_nonan.shape[1]))]]\n",
    "```\n",
    "\n",
    "Cluster 2\n",
    "\n",
    "```python\n",
    "mumbai_data_nonan.loc[mumbai_data_nonan['Cluster Labels'] == 2, mumbai_data_nonan.columns[[1] + list(range(5, mumbai_data_nonan.shape[1]))]]\n",
    "```\n",
    "\n",
    "Cluster 3\n",
    "\n",
    "```python\n",
    "mumbai_data_nonan.loc[mumbai_data_nonan['Cluster Labels'] == 3, mumbai_data_nonan.columns[[1] + list(range(5, mumbai_data_nonan.shape[1]))]]\n",
    "```\n",
    "\n",
    "Cluster 4\n",
    "\n",
    "```python\n",
    "mumbai_data_nonan.loc[mumbai_data_nonan['Cluster Labels'] == 4, mumbai_data_nonan.columns[[1] + list(range(5, mumbai_data_nonan.shape[1]))]]\n",
    "```\n",
    "\n",
    "Cluster 5\n",
    "\n",
    "```python\n",
    "mumbai_data_nonan.loc[mumbai_data_nonan['Cluster Labels'] == 5, mumbai_data_nonan.columns[[1] + list(range(5, mumbai_data_nonan.shape[1]))]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrHlnq6_EVSO"
   },
   "source": [
    "# 5. Results and Discussion\n",
    "\n",
    "\n",
    "Mumbai is relatively big in size geographically. It has a wide variety of cusines and eateries including French, Thai, Cambodian, Asian, Chinese etc. There are a lot of hangout spots including many Restaurants, Bars and Clubs.Different means of public transport in Mumbai which includes buses, taxies, trains and rikshaws.For leisure and sight seeing, there are a lot of Plazas, Trails, Parks, Historic sites, clothing shops, Art galleries. \n",
    "\n",
    "Overall, Mumbai seems like the relaxing vacation spot with a mix of lakes, historic spots and a wide variety of cusines to try out.\n",
    "\n",
    "The neighbourhoods of London are very mulitcultural. There are a lot of different cusines including Indian, Italian, Turkish and Chinese. London seems to take a step further in this direction by having a lot of Restaurants, bars, juice bars, coffee shops, Fish and Chips shop and Breakfast spots. It has a lot of shopping options too with that of the Flea markets, flower shops, fish markets, Fishing stores, clothing stores. The main modes of transport seem to be Buses and trains. For leisure, the neighbourhoods are set up to have lots of parks, golf courses, zoo, gyms and Historic sites.\n",
    "\n",
    "Overall, the city of London offers a multicultural, diverse and certainly an entertaining experience.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuztDbPeH2AN"
   },
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "The purpose of this project was to explore the cities of Mumbai and London and see how attractive it is to potential tourists and migrants. We explored both the cities based on their extrapolated the common venues present in each of the neighbourhoods finally concluding with clustering similar neighbourhoods together.\n",
    "\n",
    "We could see that each of the neighbourhoods in both the cities have a wide variety of experiences to offer which is unique in it's own way. The cultural diversity is quite evident which also gives the feeling of a sense of inclusion.\n",
    "\n",
    "Both LOndon and Mumbai seem to offer a vacation stay or a romantic gateaway with a lot of places to explore, beautiful landscapes and a wide variety of culture.Overall, it's upto the stakeholders to decide which experience they would prefer more and which would more to their liking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed code is available on [GitHub](https://github.com/amanattar/A-Tale-of-Two-Cities/blob/main/Tale_of_Two_Cities_Data_Science_Project.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
